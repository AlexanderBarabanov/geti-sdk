{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Deploying a project for offline inference\n",
    "In this notebook we'll show how to create a deployment for a project that can be used to run inference locally, using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating on host https://10.91.242.203/...\n",
      "Authentication successful. Cookie received.\n"
     ]
    }
   ],
   "source": [
    "# As usual we'll connnect to the platform first\n",
    "\n",
    "from sc_api_tools import SCRESTClient\n",
    "\n",
    "client = SCRESTClient(\n",
    "    host='https://10.91.242.203/',\n",
    "    username='sc-api-tools@intel.com',\n",
    "    password='Inteldemos!'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0fcbf-fe01-44e9-a810-226552d55385",
   "metadata": {},
   "source": [
    "### Selecting a project for deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 projects were found on the platform:\n",
      "\n",
      " Project: Bottle Anomaly Classification\n",
      "  Task 1: Anomaly task\n",
      "    Labels: ['Normal', 'Anomalous']\n",
      "\n",
      "\n",
      " Project: Bottle Quality Control\n",
      "  Task 1: Classification task\n",
      "    Labels: ['Good', 'Bad']\n",
      "\n",
      "\n",
      " Project: Weld classification\n",
      "  Task 1: Classification task\n",
      "    Labels: ['Good', 'bad', 'empty']\n",
      "\n",
      "\n",
      " Project: COCO animal detection demo\n",
      "  Task 1: Detection task\n",
      "    Labels: ['dog', 'horse', 'cat', 'No Object']\n",
      "\n",
      "\n",
      " Project: COCO anomalous animal demo\n",
      "  Task 1: Detection task\n",
      "    Labels: ['animal', 'No Object']\n",
      "  Task 2: Anomaly classification task\n",
      "    Labels: ['Normal', 'Anomalous']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sc_api_tools.rest_managers import ProjectManager\n",
    "\n",
    "project_manager = ProjectManager(session=client.session, workspace_id=client.workspace_id)\n",
    "projects = project_manager.list_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project\n",
    "Let's go with the project we created in notebook [004](004_create_pipeline_project_from_dataset.ipynb): `COCO anomalous animal demo`. To create a deployment, we can use the `client.deploy_project` convenience method. This will download the active (OpenVINO) models for all tasks in the project to our local machine, so that we can use them to run inference locally.\n",
    "\n",
    "> **NOTE**: Downloading the model data may take some time, especially models for anomaly tasks are on the order of 100 Mb in size so please be prepared to wait a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa81a0a5-697d-4c69-a2e5-e1e3696e1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving MO model data for Detection task...\n",
      "Retrieving MO model data for Anomaly classification task...\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = 'COCO anomalous animal demo'\n",
    "\n",
    "deployment = client.deploy_project(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b571-6da5-4618-8f3f-b8576d2d18a2",
   "metadata": {},
   "source": [
    "### Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae2630d-34e2-4cd7-86e9-43c534871ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ceb34-04a7-4645-8f39-9edcc0d21446",
   "metadata": {},
   "source": [
    "## Running inference on an image locally\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local inference on image took 353.86 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "numpy_image = cv2.imread('data/dogs.png')\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_image)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `show_image_with_annotation_scene` utility function\n",
    "2. Inspecting it's properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.utils import show_image_with_annotation_scene\n",
    "\n",
    "show_image_with_annotation_scene(numpy_image, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a21c59-bcb7-4645-94ae-ddfcb78775c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': [{'labels': [{'color': '#41405bff',\n",
      "                              'name': 'animal',\n",
      "                              'probability': 0.32638073},\n",
      "                             {'color': '#ff5662ff',\n",
      "                              'name': 'Anomalous',\n",
      "                              'probability': 1.0}],\n",
      "                  'shape': {'height': '0.310',\n",
      "                            'type': 'RECTANGLE',\n",
      "                            'width': '0.100',\n",
      "                            'x': '0.722',\n",
      "                            'y': '0.070'}},\n",
      "                 {'labels': [{'color': '#41405bff',\n",
      "                              'name': 'animal',\n",
      "                              'probability': 0.20990069},\n",
      "                             {'color': '#ff5662ff',\n",
      "                              'name': 'Anomalous',\n",
      "                              'probability': 1.0}],\n",
      "                  'shape': {'height': '0.207',\n",
      "                            'type': 'RECTANGLE',\n",
      "                            'width': '0.062',\n",
      "                            'x': '0.353',\n",
      "                            'y': '0.326'}},\n",
      "                 {'labels': [{'color': '#41405bff',\n",
      "                              'name': 'animal',\n",
      "                              'probability': 0.20136048},\n",
      "                             {'color': '#ff5662ff',\n",
      "                              'name': 'Anomalous',\n",
      "                              'probability': 1.0}],\n",
      "                  'shape': {'height': '0.176',\n",
      "                            'type': 'RECTANGLE',\n",
      "                            'width': '0.058',\n",
      "                            'x': '0.670',\n",
      "                            'y': '0.472'}},\n",
      "                 {'labels': [{'color': '#41405bff',\n",
      "                              'name': 'animal',\n",
      "                              'probability': 0.14946413},\n",
      "                             {'color': '#ff5662ff',\n",
      "                              'name': 'Anomalous',\n",
      "                              'probability': 1.0}],\n",
      "                  'shape': {'height': '0.185',\n",
      "                            'type': 'RECTANGLE',\n",
      "                            'width': '0.047',\n",
      "                            'x': '0.444',\n",
      "                            'y': '0.251'}}],\n",
      " 'kind': 'prediction',\n",
      " 'maps': []}\n"
     ]
    }
   ],
   "source": [
    "print(prediction.overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join('deployments', PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c8272-ef89-4d39-a70f-65f1ccf9cc9e",
   "metadata": {},
   "source": [
    "## Loading a saved deployment\n",
    "Loading a deployment that was previously saved to disk is easy and can be done without establishing a connection to the platform (or without even connecting to the internet, for that matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.deployment import Deployment\n",
    "\n",
    "loaded_deployment = Deployment.from_folder(PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Again, to prepare the deployment for inference make sure to send the models to CPU (or whichever device you want to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_deployment.load_inference_models(device='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4dac8-91cf-4920-8e3e-e8c575d91b46",
   "metadata": {},
   "source": [
    "That's it! Happy inferencing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
