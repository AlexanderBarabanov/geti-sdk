{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Deploying a project for offline inference\n",
    "\n",
    "In this notebook, we will show how to create a deployment for a project that can be used to run inference locally, using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\intel\\\\github\\\\geti-sdk\\\\geti_sdk_env\\\\lib\\\\site-packages\\\\geti_sdk\\\\deployment\\\\resources\\\\OVMS_README.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\deployment\\utils.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     OVMS_README_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m---> 24\u001b[0m         \u001b[43mresources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeti_sdk.deployment.resources\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOVMS_README.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'importlib.resources' has no attribute 'files'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# As usual we will connect to the platform first, using the server details from the .env file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeti_sdk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Geti\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeti_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_server_details_from_env\n\u001b[0;32m      6\u001b[0m geti_server_configuration \u001b[38;5;241m=\u001b[39m get_server_details_from_env()\n",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\__init__.py:79\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2022 Intel Corporation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# and limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mIntroduction\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Geti\n\u001b[0;32m     81\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeti\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\geti.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MediaList\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deployment\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     GetiRequestException,\n\u001b[0;32m     37\u001b[0m     GetiSession,\n\u001b[0;32m     38\u001b[0m     ServerCredentialConfig,\n\u001b[0;32m     39\u001b[0m     ServerTokenConfig,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrest_clients\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     AnnotationClient,\n\u001b[0;32m     43\u001b[0m     ConfigurationClient,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     VideoClient,\n\u001b[0;32m     50\u001b[0m )\n",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\deployment\\__init__.py:83\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2022 Intel Corporation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# and limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mIntroduction\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m   :show-inheritance:\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeployedModel\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deployment\n\u001b[0;32m     86\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\deployment\\deployed_model.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeti_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GetiSession\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeti_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrest_converters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigurationRESTConverter, ModelRESTConverter\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     generate_ovms_model_address,\n\u001b[0;32m     38\u001b[0m     generate_ovms_model_name,\n\u001b[0;32m     39\u001b[0m     target_device_is_ovms,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m MODEL_DIR_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m PYTHON_DIR_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\intel\\github\\geti-sdk\\geti_sdk_env\\lib\\site-packages\\geti_sdk\\deployment\\utils.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m     OVMS_README_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m     24\u001b[0m         resources\u001b[38;5;241m.\u001b[39mfiles(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeti_sdk.deployment.resources\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOVMS_README.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m resources\u001b[38;5;241m.\u001b[39mpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeti_sdk.deployment.resources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOVMS_README.md\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data_path:\n\u001b[0;32m     28\u001b[0m         OVMS_README_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\importlib\\resources.py:201\u001b[0m, in \u001b[0;36mpath\u001b[1;34m(package, resource)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m file_path\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mopen_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    202\u001b[0m         data \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# Not using tempfile.NamedTemporaryFile as it leads to deeper 'try'\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# blocks due to the need to close the temporary file to work on\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# Windows properly.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\importlib\\resources.py:91\u001b[0m, in \u001b[0;36mopen_binary\u001b[1;34m(package, resource)\u001b[0m\n\u001b[0;32m     89\u001b[0m reader \u001b[38;5;241m=\u001b[39m _get_resource_reader(package)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m _check_location(package)\n\u001b[0;32m     93\u001b[0m absolute_package_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(package\u001b[38;5;241m.\u001b[39m__spec__\u001b[38;5;241m.\u001b[39morigin)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:988\u001b[0m, in \u001b[0;36mopen_resource\u001b[1;34m(self, resource)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\intel\\\\github\\\\geti-sdk\\\\geti_sdk_env\\\\lib\\\\site-packages\\\\geti_sdk\\\\deployment\\\\resources\\\\OVMS_README.md'"
     ]
    }
   ],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0fcbf-fe01-44e9-a810-226552d55385",
   "metadata": {},
   "source": [
    "### Selecting a project for deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ProjectClient\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)\n",
    "projects = project_client.list_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project\n",
    "Let's go with the project we created in notebook [004](004_create_pipeline_project_from_dataset.ipynb): `COCO multitask animal demo`. To create a deployment, we can use the `geti.deploy_project` convenience method. This will download the active (OpenVINO) models for all tasks in the project to our local machine, so that we can use them to run inference locally.\n",
    "\n",
    "> **NOTE**: Downloading the model data may take some time, especially models for anomaly tasks are on the order of 100 Mb in size so please be prepared to wait a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b748042-6d21-471a-8acd-d3f360d543e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"COCO multitask animal demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c40da3-afeb-42ab-a8bc-be3c66d1053e",
   "metadata": {},
   "source": [
    "Before deploying, we need to make sure that the project is trained. Otherwise it will not contain any models to deploy, and the deployment will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5fb89-f4a2-4398-9951-8b1cefa4ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c279c-c6f1-4663-8c41-f96431212252",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17e98e-ac6c-4353-9282-23c2e20835d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = geti.deploy_project(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b571-6da5-4618-8f3f-b8576d2d18a2",
   "metadata": {},
   "source": [
    "### Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d01de-6b6a-4174-a033-562034c1374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ceb34-04a7-4645-8f39-9edcc0d21446",
   "metadata": {},
   "source": [
    "## Running inference on an image locally\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it.\n",
    "The SDK contains an example image that we use for this. The path to the image is in the `EXAMPLE_IMAGE_PATH` constant, from the `geti_sdk.demos` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from geti_sdk.demos import EXAMPLE_IMAGE_PATH\n",
    "\n",
    "numpy_image = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
    "\n",
    "# Convert to RGB channel order. All deployed models expect the image in RGB format\n",
    "numpy_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `show_image_with_annotation_scene` utility function\n",
    "2. Inspecting its properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.utils import show_image_with_annotation_scene\n",
    "\n",
    "show_image_with_annotation_scene(numpy_image, prediction, show_in_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21c59-bcb7-4645-94ae-ddfcb78775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction.overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join(\"deployments\", PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c8272-ef89-4d39-a70f-65f1ccf9cc9e",
   "metadata": {},
   "source": [
    "## Loading a saved deployment\n",
    "Loading a deployment that was previously saved to disk is easy and can be done without establishing a connection to the platform (or without even connecting to the internet, for that matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.deployment import Deployment\n",
    "\n",
    "offline_deployment = Deployment.from_folder(PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Again, to prepare the deployment for inference make sure to send the models to CPU (or whichever device you want to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4dac8-91cf-4920-8e3e-e8c575d91b46",
   "metadata": {},
   "source": [
    "That's all there is to it! The `offline_deployment` can now be used to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5570fd5-3b6d-4fee-9104-8747ab38e6ca",
   "metadata": {},
   "source": [
    "# Comparing local inference and inference on the platform\n",
    "As a final step, we can make a comparison between the local inference results and the predictions sent back from the platform. We will have a look at the time required for both methods, and compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8293cec-743f-459a-8634-f0d87b0b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ImageClient, PredictionClient\n",
    "\n",
    "project = project_client.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "image_client = ImageClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")\n",
    "prediction_client = PredictionClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c2b5-ecb9-4a47-bafc-c6fe136bfb1d",
   "metadata": {},
   "source": [
    "To prepare for platform inference, we have to upload the image to the platform first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca9c4d-a904-4d9a-bf42-5c84d83cf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_image = image_client.upload_image(numpy_image)\n",
    "# Load the pixel data to visualize the image later on\n",
    "sc_image.get_data(geti.session);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef59a6-baf8-411f-a01b-e0620eee7348",
   "metadata": {},
   "source": [
    "### Comparing inference times\n",
    "Now, we can run inference locally and on the platform, and time both. We will set the prediction client to `ONLINE` mode, which means it will always generate a new prediction for the image, rather than returning cached predictions. Additionally you can set the mode to `AUTO` (which will return cached predictions if available) and re-run the cell to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322cd4c-4fb1-42c5-9fa9-547d286b7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.data_models.enums import PredictionMode\n",
    "\n",
    "prediction_client.mode = PredictionMode.ONLINE\n",
    "\n",
    "# Get platform prediction, and measure time required\n",
    "t_start_platform = time.time()\n",
    "platform_prediction = prediction_client.get_image_prediction(sc_image)\n",
    "t_elapsed_platform = time.time() - t_start_platform\n",
    "\n",
    "# Get local prediction, and measure time required\n",
    "t_start_local = time.time()\n",
    "local_prediction = offline_deployment.infer(numpy_rgb)\n",
    "t_elapsed_local = time.time() - t_start_local\n",
    "\n",
    "print(f\"Platform prediction completed in {t_elapsed_platform*1000:.1f} milliseconds\")\n",
    "print(f\"Local prediction completed in {t_elapsed_local*1000:.1f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a17fc3-bcb9-48d1-811f-c02969bf60a2",
   "metadata": {},
   "source": [
    "### Comparing inference results\n",
    "The cell below will show the results from the platform prediction (top) and local prediction (bottom). The two predictions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f55ff1-f2ca-4b2d-9dbe-d4e09d6c2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_annotation_scene(sc_image, platform_prediction, show_in_notebook=True)\n",
    "show_image_with_annotation_scene(numpy_image, local_prediction, show_in_notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05880bf6-43d3-4ecc-afea-39ef5e0597ae",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up, we will delete the sc_image from the project again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26681cc5-fc42-4412-8ea5-af5d57201c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_client.delete_images([sc_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5163a7-379d-479f-94bd-83cfbb5e3307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
