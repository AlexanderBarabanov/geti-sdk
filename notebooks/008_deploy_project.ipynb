{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Deploying a project for offline inference\n",
    "\n",
    "> **WARNING**: The deployment-related features of this package are in an experimental\n",
    "> stage, and subject to change.\n",
    "\n",
    "In this notebook we'll show how to create a deployment for a project that can be used to run inference locally, using OpenVINO\n",
    "\n",
    "> **NOTE**: Before running this notebook, please make sure that you've installed the required packages for local deployment. If you haven't already done so, you can install them by taking the following \n",
    "> steps:\n",
    "> 1. Open a terminal\n",
    "> 2. Navigate to the root folder of the sc-api-tools repo, i.e. one level up from the /notebooks folder\n",
    "> 3. Activate the python environment you are using to run sc-api-tools\n",
    "> 3. Run the command `pip install -r requirements-deployment.txt` to install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual we'll connnect to the platform first, using the credentials from the .env file.\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from sc_api_tools import SCRESTClient\n",
    "\n",
    "env_variables = dotenv_values(dotenv_path=\".env\")\n",
    "\n",
    "if not env_variables:\n",
    "    print(\"Unable to load login details from .env file, please make sure the file exists at the root of the notebooks directory.\")\n",
    "\n",
    "client = SCRESTClient(\n",
    "    host=env_variables.get('HOST'),\n",
    "    username=env_variables.get('USERNAME'),\n",
    "    password=env_variables.get('PASSWORD')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0fcbf-fe01-44e9-a810-226552d55385",
   "metadata": {},
   "source": [
    "### Selecting a project for deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.rest_managers import ProjectManager\n",
    "\n",
    "project_manager = ProjectManager(session=client.session, workspace_id=client.workspace_id)\n",
    "projects = project_manager.list_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project\n",
    "Let's go with the project we created in notebook [004](004_create_pipeline_project_from_dataset.ipynb): `COCO anomalous animal demo`. To create a deployment, we can use the `client.deploy_project` convenience method. This will download the active (OpenVINO) models for all tasks in the project to our local machine, so that we can use them to run inference locally.\n",
    "\n",
    "> **NOTE**: Downloading the model data may take some time, especially models for anomaly tasks are on the order of 100 Mb in size so please be prepared to wait a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81a0a5-697d-4c69-a2e5-e1e3696e1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'COCO anomalous animal demo'\n",
    "\n",
    "deployment = client.deploy_project(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b571-6da5-4618-8f3f-b8576d2d18a2",
   "metadata": {},
   "source": [
    "### Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2630d-34e2-4cd7-86e9-43c534871ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ceb34-04a7-4645-8f39-9edcc0d21446",
   "metadata": {},
   "source": [
    "## Running inference on an image locally\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "numpy_image = cv2.imread('data/dogs.png')\n",
    "\n",
    "# Convert to RGB channel order. All deployed models expect the image in RGB format\n",
    "numpy_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `show_image_with_annotation_scene` utility function\n",
    "2. Inspecting it's properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.utils import show_image_with_annotation_scene\n",
    "\n",
    "show_image_with_annotation_scene(numpy_image, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21c59-bcb7-4645-94ae-ddfcb78775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction.overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join('deployments', PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c8272-ef89-4d39-a70f-65f1ccf9cc9e",
   "metadata": {},
   "source": [
    "## Loading a saved deployment\n",
    "Loading a deployment that was previously saved to disk is easy and can be done without establishing a connection to the platform (or without even connecting to the internet, for that matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.deployment import Deployment\n",
    "\n",
    "offline_deployment = Deployment.from_folder(PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Again, to prepare the deployment for inference make sure to send the models to CPU (or whichever device you want to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_deployment.load_inference_models(device='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4dac8-91cf-4920-8e3e-e8c575d91b46",
   "metadata": {},
   "source": [
    "That's all there is to it! The `offline_deployment` can now be used to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5570fd5-3b6d-4fee-9104-8747ab38e6ca",
   "metadata": {},
   "source": [
    "# Comparing local inference and inference on the platform\n",
    "As a final step, we can make a comparison between the local inference results and the predictions sent back from the platform. We'll have a look at the time required for both methods, and compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8293cec-743f-459a-8634-f0d87b0b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.rest_managers import ImageManager, PredictionManager\n",
    "\n",
    "project = project_manager.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "image_manager = ImageManager(session=client.session, workspace_id=client.workspace_id, project=project)\n",
    "prediction_manager = PredictionManager(session=client.session, workspace_id=client.workspace_id, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c2b5-ecb9-4a47-bafc-c6fe136bfb1d",
   "metadata": {},
   "source": [
    "To prepare for platform inference, we have to upload the image to the platform first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca9c4d-a904-4d9a-bf42-5c84d83cf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_image = image_manager.upload_image(numpy_image)\n",
    "# Load the pixel data to visualize the image later on\n",
    "sc_image.get_data(client.session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef59a6-baf8-411f-a01b-e0620eee7348",
   "metadata": {},
   "source": [
    "### Comparing inference times\n",
    "Now, we can run inference locally and on the platform, and time both. We'll set the prediction manager to `ONLINE` mode, which means it will always generate a new prediction for the image, rather than returning cached predictions. Additionally you can set the mode to `AUTO` (which will return cached predictions if available) and re-run the cell to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322cd4c-4fb1-42c5-9fa9-547d286b7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_api_tools.data_models.enums import PredictionMode\n",
    "\n",
    "prediction_manager.mode = PredictionMode.ONLINE\n",
    "\n",
    "# Get platform prediction, and measure time required\n",
    "t_start_platform = time.time()\n",
    "platform_prediction = prediction_manager.get_image_prediction(sc_image)\n",
    "t_elapsed_platform = time.time() - t_start_platform\n",
    "\n",
    "# Get local prediction, and measure time required\n",
    "t_start_local = time.time()\n",
    "local_prediction = offline_deployment.infer(numpy_rgb)\n",
    "t_elapsed_local = time.time() - t_start_local\n",
    "\n",
    "print(f'Platform prediction completed in {t_elapsed_platform*1000:.1f} milliseconds')\n",
    "print(f'Local prediction completed in {t_elapsed_local*1000:.1f} milliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a17fc3-bcb9-48d1-811f-c02969bf60a2",
   "metadata": {},
   "source": [
    "### Comparing inference results\n",
    "The cell below will first show a pop up window with the predictions from the platform, and after that a window showing the local predictions. The two predictions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f55ff1-f2ca-4b2d-9dbe-d4e09d6c2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_annotation_scene(sc_image, platform_prediction)\n",
    "show_image_with_annotation_scene(numpy_image, local_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05880bf6-43d3-4ecc-afea-39ef5e0597ae",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up, we'll delete the sc_image from the project again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26681cc5-fc42-4412-8ea5-af5d57201c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_manager.delete_images([sc_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da474a-5690-4798-90d0-180e4cb5fd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
