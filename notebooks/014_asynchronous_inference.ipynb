{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b328bbe-274d-4f76-8e71-f7f2ed17ebf9",
   "metadata": {},
   "source": [
    "# Asynchronous inference\n",
    "In this notebook, we will configure a model deployment for asynchronous inference.\n",
    "This notebook assumes that you are familiar with model deployments for local inference (if not, please have a look at [notebook 008](008_deploy_project.ipynb) first). \n",
    "\n",
    "## Synchronous vs Asynchronous inference\n",
    "First things first, what is asynchronous inference and why bother to use it? Up to now, we've been strictly using *synchronous* code to run local inference for our Geti models. This means that whenever we make an infer request to our model (using `deployment.infer()`), the code execution blocks and waits for the model to process the image that is being inferred. Since this is a compute-intensive operation, the CPU (or whatever device we use for inference) will be fully occupied at this time to perform the required calculations to compute the model activations for the image that we feed it. So far, so good. \n",
    "\n",
    "However, things may be different if you are running your inference code on a machine with multiple CPU cores. In that case, the synchronous call to `deployment.infer()` will fully occupy one of the cores, but leave the others running idle. The reason is that we can't efficiently share memory between processes running on different cores (this introduces overhead), so the computations for a single image to be inferred cannot simply be distributed across all CPU cores. \n",
    "\n",
    "If you only care about *latency*, meaning you want to get a result for your single image as quickly as possible, then this is not an issue. However, if *throughput* is an issue (for example for video processing), we may be able to improve the situation by using parallel processing. Instead of blocking execution for each infer request and waiting for it to complete, we can already send the *next* frame to another CPU core that would otherwise by sitting idle. This puts both cores to work at the same time, thereby increasing the rate at which the frames can be processed. This is exactly what we refer to as *asynchronous inference*.\n",
    "\n",
    "Luckily OpenVINO takes care of the parallelization and optimization of this process for us, we just have to set up our code for running local model inference a bit differently.\n",
    "\n",
    "## Contents of this notebook\n",
    "In this notebook we will go through the following steps:\n",
    "1. Create a deployment for a Geti project\n",
    "2. Prepare the deployment for *asynchronous* inference\n",
    "3. Run a benchmark to measure the inference rate\n",
    "4. Switch to *synchronous* inference mode\n",
    "5. Benchmark again and compare the async and sync inference rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7c4cd-761f-482f-8ffa-a43cad2c4c92",
   "metadata": {},
   "source": [
    "## Step 1: Create deployment\n",
    "Let's connect to Geti and create the deployment for any project. Here, we'll use the project from [notebook 004](004_create_pipeline_project_from_dataset.ipynb) again, `COCO multitask animal demo`.\n",
    "\n",
    "This is a multi-task project with a detection and classification task. If you don't have it yet on your Geti instance, you can run notebook 004 to create it. Or, you can use one of your own projects instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c0f1b-07c4-449a-b926-2dd0839f9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "PROJECT_NAME = \"COCO multitask animal demo\"\n",
    "project = geti.get_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4dc62-841a-4fa8-b428-038c33ce4fba",
   "metadata": {},
   "source": [
    "Now, let's deploy the project and save the deployment for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e4b7f-c735-4def-b4b4-62e51c96f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_FOLDER = \"deployments\"\n",
    "\n",
    "deployment = geti.deploy_project(PROJECT_NAME, output_folder=DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e356a-ff43-4533-be2c-8fb9d6d26acf",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the deployment for asynchronous inference\n",
    "To use the deployment in asynchronous mode, there are two main things to consider:\n",
    "1. Upon loading the inference models, we need to specify the size of the `infer queue` for the model. The infer queue is essentially a space of shared memory in which infer requests are stored. A request will be in the queue until one of the machine's cores is ready to process it. A larger queue means that requests may be picked up more rapidly, but will also consume more of the available system memory. Usually, setting the queue size to be roughly equal to the number of CPU cores on your system is a good choice.\n",
    "2. Defining what should happen when an infer request has finished processing. This is done via a function referred to as a `callback`. The callback executes whenever an infer request is ready, and the results are available. In this notebook, we'll set up a callback to print the inference results to the screen and save our image (with prediction overlay) to a folder on disk.\n",
    "\n",
    "First of all, let's load the inference models. We'll set the number of infer requests (the infer queue) to be equal to the number of cores on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f25327-eda0-4843-80d3-48b332f4b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Detected {num_cores} cpu cores.\")\n",
    "\n",
    "deployment.load_inference_models(\n",
    "    device=\"CPU\",\n",
    "    max_async_infer_requests=num_cores,\n",
    "    openvino_configuration={\"PERFORMANCE_HINT\": \"THROUGHPUT\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b197-c8fd-41d0-8ab6-b3ebf75252b1",
   "metadata": {},
   "source": [
    "You should see some output showing that the models in the deployment are loaded to CPU, with the number of infer requests set equal to the number of CPU cores.\n",
    "\n",
    "Now, let's define a `callback` function to handle the inference results. The callback function has a particular signature. It should take as it's arguments:\n",
    "- The `image` or video frame that was inferred, as a numpy array\n",
    "- The `prediction`, which is the result of the model inference\n",
    "- Any additional `runtime_data`, which was passed along with the infer request\n",
    "\n",
    "The first two arguments are always the same, the image as a numpy array and the resulst as a `Prediction` object. However, the runtime data is more flexible. We can decide what we pass here, it can be anything that we want to use in the callback to further process our results. For example, a filename, timestamp, index, etc. In this example we will simply use the image index. The callback should not return any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf35ecf-5cf1-4751-ac6a-d1a8b9fdff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from geti_sdk.data_models import Prediction\n",
    "from geti_sdk.utils import show_image_with_annotation_scene\n",
    "\n",
    "# First, we'll specify the output folder and make sure it exists\n",
    "output_folder = \"output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def handle_results(image: np.ndarray, result: Prediction, runtime_data: int) -> None:\n",
    "    \"\"\"\n",
    "    Handles asynchronous inference results. Gets called after completion of each infer request.\n",
    "    \"\"\"\n",
    "    # First, save the image in the `output_folder`,\n",
    "    filepath = os.path.join(output_folder, f\"result_{runtime_data}.jpg\")\n",
    "    show_image_with_annotation_scene(image, result, filepath)\n",
    "\n",
    "    # Print the number of predicted objects, and the probability score for each label in each object\n",
    "    predicted_objects = result.annotations\n",
    "    print(f\"Image {runtime_data} contains {len(predicted_objects)} objects:\")\n",
    "    for obj in predicted_objects:\n",
    "        label_mapping = {lab.name: lab.probability for lab in obj.labels}\n",
    "        print(f\"    {label_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79037677-1bf8-47ee-8aa8-eeb854eb2c61",
   "metadata": {},
   "source": [
    "Now that we have defined the callback, we need to assign it to the deployment. This will switch the deployment over to asynchronous mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52b3a7-4eb7-4333-bd4a-88d2cdca7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.set_asynchronous_callback(handle_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88036f-e520-44a1-8ffc-ea04fcf1d7ce",
   "metadata": {},
   "source": [
    "If all goes well, you should see a log line output stating that asynchronous inference mode has been enabled. Now, we are ready to infer!\n",
    "\n",
    "## Step 3: Run a benchmark to measure inference rate\n",
    "The next section shows how to run inference in asynchronous mode. We will run inference on 50 images from the COCO dataset. In the next cell, we'll select the filepaths for the images to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3285-a288-4efe-94a0-d1579ab598f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.annotation_readers import DatumAnnotationReader\n",
    "from geti_sdk.demos import get_coco_dataset\n",
    "\n",
    "n_images = 50\n",
    "\n",
    "path = get_coco_dataset()\n",
    "ar = DatumAnnotationReader(path, annotation_format=\"coco\")\n",
    "ar.filter_dataset(labels=[\"dog\", \"horse\", \"elephant\"])\n",
    "coco_image_filenames = ar.get_all_image_names()\n",
    "coco_image_filepaths = [\n",
    "    os.path.join(path, \"images\", \"val2017\", fn + \".jpg\") for fn in coco_image_filenames\n",
    "][0:n_images]\n",
    "print(f\"Selected {n_images} images from COCO dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520dd5e-2b17-47fe-a9d9-ac54f708a90c",
   "metadata": {},
   "source": [
    "Now, we're ready to run the benchmark! Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320912a-c7c1-456a-84ed-d0601a765ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "t_start_async = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    deployment.infer_async(img_rgb, img_index)\n",
    "\n",
    "# Wait until inference completes\n",
    "deployment.await_all()\n",
    "t_elapsed_async = time.time() - t_start_async\n",
    "\n",
    "print(\n",
    "    f\"Asynchronous mode: Inferred {len(coco_image_filepaths)} images in {t_elapsed_async:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_async:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1cac2-cf7e-466a-aaa2-0bb74988c24e",
   "metadata": {},
   "source": [
    "You should see the model output printed on the screen for each image. The model detects animals, and classifies them as `wild` or `domestic`. In the printed output, it shows the number of objects (animals) for each image, as well as the labels for each object and the probability associated with it.\n",
    "\n",
    "In addition, your workspace should now contain a folder called `output`, which contains the result overlay for each image. Each file should be named `result_x.jpg`, where `x` is the index of the image. \n",
    "\n",
    "Finally, at the bottom of the printed output you should see a line stating the time it took to run the inference for all images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13e610-23c2-4a94-9a4a-efed0811319d",
   "metadata": {},
   "source": [
    "## Step 4: Switch to *synchronous* execution mode\n",
    "Let's switch back to the familiar synchronous inference mode. The deployment provides a simple method to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847923a-aff1-463f-b946-781ecc70eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.asynchronous_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dbc1c1-0b74-4636-b401-25980ed1cf6b",
   "metadata": {},
   "source": [
    "This removes any callback function that we set and allows us to use the regular `deployment.infer` method again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3876ce-6986-4210-b926-ae026b68b722",
   "metadata": {},
   "source": [
    "## Step 5: Running the benchmark in synchronous mode\n",
    "Now, let's run the same inference code in synchronous execution mode and compare the time required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10c7b8-2113-4a90-9322-01afdcff81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_sync = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # We modify this part only: use `infer` instead of `infer_async`\n",
    "    result = deployment.infer(img_rgb)\n",
    "    # Manually call the function that we defined to handle model results\n",
    "    handle_results(image=img_rgb, result=result, runtime_data=img_index)\n",
    "\n",
    "# No need to wait anymore, in synchronous mode the code will not stop until all images are inferred\n",
    "t_elapsed_sync = time.time() - t_start_sync\n",
    "\n",
    "print(\n",
    "    f\"Synchronous mode: Inferred {len(coco_image_filepaths)} images in {t_elapsed_sync:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_sync:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5978c5d-1ff7-42ce-81d8-16740dcb7841",
   "metadata": {},
   "source": [
    "You should see the same output as before, with the number of objects and probabilities per label printed per image.\n",
    "Also, the time required for the whole process is printed on the last line, like before. Let's have a look at the speedup we get from using the asynchronous mode by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda95ac-3bbc-49df-8b51-471f01432f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Sychronous mode: Time elapsed is {t_elapsed_sync:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_sync:.1f} fps)\"\n",
    ")\n",
    "print(\n",
    "    f\"Asychronous mode: Time elapsed is {t_elapsed_async:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_async:.1f} fps)\"\n",
    ")\n",
    "print(\n",
    "    f\"Asynchronous inference is {t_elapsed_sync/t_elapsed_async:.1f} times faster than synchronous inference.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122cb25-93f2-4bda-8cc1-7cf6419c93a3",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Clearly, asynchronous mode gives a better speedup if you have more cores available. Also, if you care mostly about latency (i.e. minimal inference time for a single image) it is probably not the way to go, since the inference time for a single image can increase a bit due to the added overhead of the asynchronous processing. However, if you care mostly about the average inference time over a lot of images, asynchronous mode will almost always provide an increased inference rate compared to synchronous mode.\n",
    "\n",
    "One thing you may have noticed is that in asynchronous mode, the output is not necessarily printed in order. Results for images at different indexes might be mixed up, because they are processed in parallel and one might take longer than another. If you simply want to infer a folder with a lot of images this is most likely not a problem, however for applications where the order of the images does matter (for example in video processing) extra care needs to be taken to re-order the frames once inference is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5adfa8-c77b-4b04-a59f-76429dea06f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
